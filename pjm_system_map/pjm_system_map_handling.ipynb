{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOS 6-4-2020\n",
    "### 1. Get better generator data. Currently missing a lot of coal and nuclear\n",
    "### 2. Figure out how to estimate lines reactance and apparent power limit\n",
    "### 3. Map Generator, Loads, and Transmission Lines to LMP Pnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from functools import partial\n",
    "from time import sleep\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import pypsa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "from geovoronoi import coords_to_points, points_to_coords, voronoi_regions_from_coords, calculate_polygon_areas\n",
    "from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points: 732\n",
      "passed data check\n",
      "finished constructing geojson\n",
      "geojson file has been written out to disk!\n",
      "geojson file has been read as a geodataframe!\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "with open('data/pjm_system_map_export/backbone_lines.json') as f:\n",
    "    data = json.load(f)\n",
    "print(\"number of data points:\", len(data[\"results\"]))\n",
    "\n",
    "# check data\n",
    "for x in data[\"results\"]:\n",
    "    if x[\"layerId\"] != 9:\n",
    "        raise(\"Failed layerId check!\")\n",
    "    if x[\"layerName\"] != \"Backbone Transmission Lines\":\n",
    "        raise(\"Failed layerName check!\")\n",
    "    if x[\"geometryType\"] != \"esriGeometryPolyline\":\n",
    "        raise(\"Failed geometryType check!\")\n",
    "    if len(x[\"geometry\"][\"paths\"]) != 1:\n",
    "        raise(\"Failed paths length check!\")\n",
    "print(\"passed data check\")\n",
    "\n",
    "\n",
    "\n",
    "# # load data\n",
    "# with open('data/pjm_system_map_export/non_backbone_lines_120_138_161_230_kv.json') as f:\n",
    "#     data2 = json.load(f)\n",
    "# print(\"number of data points:\", len(data2[\"results\"]))\n",
    "\n",
    "# # check data\n",
    "# for x in data2[\"results\"]:\n",
    "#     if x[\"layerId\"] != 10:\n",
    "#         raise(\"Failed layerId check!\")\n",
    "#     if x[\"layerName\"] != \"Transmission Lines\":\n",
    "#         raise(\"Failed layerName check!\")\n",
    "#     if x[\"geometryType\"] != \"esriGeometryPolyline\":\n",
    "#         raise(\"Failed geometryType check!\")\n",
    "#     if len(x[\"geometry\"][\"paths\"]) != 1:\n",
    "#         raise(\"Failed paths length check!\")\n",
    "# print(\"passed data check\")\n",
    "\n",
    "# # add to data\n",
    "# data[\"results\"] += data2[\"results\"]\n",
    "\n",
    "\n",
    "# # load data\n",
    "# with open('data/pjm_system_map_export/non_backbone_lines_69_115_kv.json') as f:\n",
    "#     data3 = json.load(f)\n",
    "# print(\"number of data points:\", len(data3[\"results\"]))\n",
    "\n",
    "# # check data\n",
    "# for x in data3[\"results\"]:\n",
    "#     if x[\"layerId\"] != 10:\n",
    "#         raise(\"Failed layerId check!\")\n",
    "#     if x[\"layerName\"] != \"Transmission Lines\":\n",
    "#         raise(\"Failed layerName check!\")\n",
    "#     if x[\"geometryType\"] != \"esriGeometryPolyline\":\n",
    "#         raise(\"Failed geometryType check!\")\n",
    "#     if len(x[\"geometry\"][\"paths\"]) != 1:\n",
    "#         raise(\"Failed paths length check!\")\n",
    "# print(\"passed data check\")\n",
    "\n",
    "\n",
    "# # add to data\n",
    "# data[\"results\"] += data3[\"results\"]\n",
    "\n",
    "\n",
    "# construct geojson\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"crs\": { \"type\": \"name\", \"properties\": { \"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\" } }, # this is not the appropriate projection\n",
    "    \"features\": [\n",
    "    {\n",
    "        \"type\": \"Feature\",\n",
    "        \"properties\" : d[\"attributes\"],\n",
    "        \"geometry\" : {\n",
    "            \"type\": \"LineString\",\n",
    "            \"coordinates\": d[\"geometry\"][\"paths\"][0],\n",
    "            }\n",
    "     } for d in data[\"results\"]]\n",
    "}\n",
    "\n",
    "print(\"finished constructing geojson\")\n",
    "\n",
    "# write geojson to disk\n",
    "output = open(\"data/pjm_system_map_export/lines.geojson\", 'w')\n",
    "json.dump(geojson, output)\n",
    "output.close()\n",
    "print(\"geojson file has been written out to disk!\")\n",
    "\n",
    "# load geojson as a geodataframe\n",
    "lines = gpd.read_file(\"data/pjm_system_map_export/lines.geojson\")\n",
    "print(\"geojson file has been read as a geodataframe!\")\n",
    "\n",
    "# replace NULL and None values with NaN\n",
    "lines = lines.replace({\"Null\": np.nan, None: np.nan, \"\":np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines.geometry.plot(figsize=(25, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line_rating = pd.read_csv(\"data/oasis/line_rating.csv\")\n",
    "\n",
    "# node_list = pd.read_excel(\"data/lmp-bus-model.xlsx\", skiprows=2)\n",
    "# node_list.Voltage = node_list.Voltage.apply(lambda x: float(x.replace(\"KV\", \"\")))\n",
    "# node_list.columns = [\"pnode_id\", \"zone\", \"substation\", \"voltage\", \"equipment\", \"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_found = []\n",
    "\n",
    "# for x in line_rating.substation.unique():\n",
    "#     if x not in node_list.substation.to_list():\n",
    "#         not_found.append(x)\n",
    "\n",
    "# len(not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines[lines[\"TO_LINE_NAME\"] == \"Null\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Substation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points: 6341\n",
      "passed data check\n",
      "number of data points: 6532\n",
      "passed data check\n",
      "finished constructing geojson\n",
      "geojson file has been written out to disk!\n",
      "geojson file has been read as a geodataframe!\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "with open('data/pjm_system_map_export/substation.json') as f:\n",
    "    data = json.load(f)\n",
    "print(\"number of data points:\", len(data[\"results\"]))\n",
    "\n",
    "\n",
    "# check data\n",
    "for x in data[\"results\"]:\n",
    "    if x[\"layerId\"] != 2:\n",
    "        raise(\"Failed layerId check!\")\n",
    "    if x[\"layerName\"] != \"Substations\":\n",
    "        raise(\"Failed layerName check!\")\n",
    "    if x[\"geometryType\"] != \"esriGeometryPoint\":\n",
    "        raise(\"Failed geometryType check!\")\n",
    "print(\"passed data check\")\n",
    "\n",
    "\n",
    "# load data\n",
    "with open('data/pjm_system_map_export/substation_non_member.json') as f:\n",
    "    data2 = json.load(f)\n",
    "print(\"number of data points:\", len(data2[\"results\"]))\n",
    "\n",
    "\n",
    "# check data\n",
    "for x in data2[\"results\"]:\n",
    "    if x[\"layerId\"] != 26:\n",
    "        raise(\"Failed layerId check!\")\n",
    "    if x[\"layerName\"] != \"Non-Member Substations\":\n",
    "        raise(\"Failed layerName check!\")\n",
    "    if x[\"geometryType\"] != \"esriGeometryPoint\":\n",
    "        raise(\"Failed geometryType check!\")\n",
    "print(\"passed data check\")\n",
    "\n",
    "# add to data\n",
    "data[\"results\"] += data2[\"results\"]\n",
    "\n",
    "\n",
    "# construct geojson\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"crs\": { \"type\": \"name\", \"properties\": { \"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\" } }, # this is not the appropriate projection\n",
    "    \"features\": [\n",
    "    {\n",
    "        \"type\": \"Feature\",\n",
    "        \"properties\" : d[\"attributes\"],\n",
    "        \"geometry\" : {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [d[\"geometry\"][\"x\"], d[\"geometry\"][\"y\"]],\n",
    "            }\n",
    "     } for d in data[\"results\"]]\n",
    "}\n",
    "\n",
    "print(\"finished constructing geojson\")\n",
    "\n",
    "\n",
    "# write geojson to disk\n",
    "output = open(\"data/pjm_system_map_export/substation.geojson\", 'w')\n",
    "json.dump(geojson, output)\n",
    "output.close()\n",
    "print(\"geojson file has been written out to disk!\")\n",
    "\n",
    "# load geojson as a geodataframe\n",
    "substations = gpd.read_file(\"data/pjm_system_map_export/substation.geojson\")\n",
    "print(\"geojson file has been read as a geodataframe!\")\n",
    "\n",
    "# replace NULL and None values with NaN\n",
    "substations = substations.replace({\"Null\": np.nan, None: np.nan, \"\":np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substations.geometry.plot(figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "\n",
      "nan\n",
      "{C1B8B982-30DB-4CB0-8ABE-07AA5ED59FE9}\n",
      "{5DA81B32-4CF2-4253-BEAD-4E60375DBB6C}\n",
      "{14AD9BDD-513B-439D-9DEB-DBE59F014A88}\n"
     ]
    }
   ],
   "source": [
    "# check if any substations in lines can't be found in the substation list\n",
    "\n",
    "for x in lines.SUBSTATION_A_GLOBALID.unique():\n",
    "    if x not in substations.SUBSTATION_GLOBALID.unique():\n",
    "        print(x)\n",
    "\n",
    "print()\n",
    "\n",
    "for x in lines.SUBSTATION_B_GLOBALID.unique():\n",
    "    if x not in substations.SUBSTATION_GLOBALID.unique():\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Substations in System Map to PNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points: 6343\n",
      "passed data check\n",
      "finished constructing geojson\n",
      "geojson file has been written out to disk!\n",
      "geojson file has been read as a geodataframe!\n"
     ]
    }
   ],
   "source": [
    "# load substation labels\n",
    "\n",
    "with open('data/pjm_system_map_export/substation_labels.json') as f:\n",
    "    data = json.load(f)\n",
    "print(\"number of data points:\", len(data[\"results\"]))\n",
    "\n",
    "\n",
    "# check data\n",
    "for x in data[\"results\"]:\n",
    "    if x[\"layerId\"] != 3:\n",
    "        raise(\"Failed layerId check!\")\n",
    "    if x[\"geometryType\"] != \"esriGeometryPoint\":\n",
    "        raise(\"Failed geometryType check!\")\n",
    "print(\"passed data check\")\n",
    "\n",
    "\n",
    "# construct geojson\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"crs\": { \"type\": \"name\", \"properties\": { \"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\" } }, # this is not the appropriate projection\n",
    "    \"features\": [\n",
    "    {\n",
    "        \"type\": \"Feature\",\n",
    "        \"properties\" : d[\"attributes\"],\n",
    "        \"geometry\" : {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [d[\"geometry\"][\"x\"], d[\"geometry\"][\"y\"]],\n",
    "            }\n",
    "     } for d in data[\"results\"]]\n",
    "}\n",
    "\n",
    "print(\"finished constructing geojson\")\n",
    "\n",
    "\n",
    "# write geojson to disk\n",
    "output = open(\"data/pjm_system_map_export/substation_labels.geojson\", 'w')\n",
    "json.dump(geojson, output)\n",
    "output.close()\n",
    "print(\"geojson file has been written out to disk!\")\n",
    "\n",
    "# load geojson as a geodataframe\n",
    "substation_labels = gpd.read_file(\"data/pjm_system_map_export/substation_labels.geojson\")\n",
    "print(\"geojson file has been read as a geodataframe!\")\n",
    "\n",
    "# replace NULL and None values with NaN\n",
    "substation_labels = substation_labels.replace({\"Null\": np.nan, None: np.nan, \"\":np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zone by zone and voltage level by voltage level\n",
    "\n",
    "# load pnode_list\n",
    "node_list = pd.read_excel(\"data/lmp-bus-model.xlsx\", skiprows=2)\n",
    "node_list.Voltage = node_list.Voltage.apply(lambda x: float(x.replace(\"KV\", \"\")))\n",
    "node_list.columns = [\"pnode_id\", \"zone\", \"substation\", \"voltage\", \"equipment\", \"type\"]\n",
    "node_list.substation = node_list.substation.astype(str)\n",
    "\n",
    "\n",
    "# replace zonal name to accomodate matching with equiplist\n",
    "zonalMap = {'AECO': \"AEC\",\n",
    "            'AEP': \"AEP\",\n",
    "            'APS': \"APS\",\n",
    "            'ATSI': \"ATSI\",\n",
    "            'BGE': \"BGE\",\n",
    "            'COMED': \"ComEd\",\n",
    "            'DAY': \"Dayton\",\n",
    "            'DEOK': \"DEOK\",\n",
    "            'DOM': \"Dominion\",\n",
    "            'DPL': \"DPL\",\n",
    "            'DUQ': \"DL\",\n",
    "            'EKPC': \"EKPC\",\n",
    "            'JCPL': \"JCPL\",\n",
    "            'METED': \"ME\",\n",
    "            'OVEC': \"OVEC HQ\",\n",
    "            'PECO': \"PECO\",\n",
    "            'PENELEC': \"PENELEC\",\n",
    "            'PEPCO': \"PEPCO\",\n",
    "            'PPL': \"PPL\",\n",
    "            'PSEG': \"PSEG\",\n",
    "            'RECO': \"RE\"}\n",
    "\n",
    "node_list.zone = node_list.zone.replace(zonalMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that zone are one-one match between the two dataframes\n",
    "\n",
    "for x in node_list.zone.unique():\n",
    "    if x not in substation_labels.PLANNING_ZONE_NAME.to_list():\n",
    "        print(x)\n",
    "\n",
    "for x in substation_labels.PLANNING_ZONE_NAME.dropna().unique():\n",
    "    if x not in node_list.zone.to_list():\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of zones: 21\n",
      "geojson file has been written out to disk!\n",
      "geojson file has been read as a geodataframe!\n"
     ]
    }
   ],
   "source": [
    "# convert raw pjm zone json data exported from system map to geojson for geopandas ingesting\n",
    "\n",
    "# load pjm zones\n",
    "with open('data/pjm_system_map_export/pjm_zone.json') as f:\n",
    "    zone = json.load(f)\n",
    "    \n",
    "print(\"number of zones:\", len(zone[\"results\"]))\n",
    "\n",
    "\n",
    "# check data\n",
    "for x in zone[\"results\"]:\n",
    "    if x[\"layerId\"] != 17:\n",
    "        raise(\"Failed layerId check!\")\n",
    "    if x[\"layerName\"] != \"PJM Zones\":\n",
    "        raise(\"Failed layerName check!\")\n",
    "    if x[\"geometryType\"] != \"esriGeometryPolygon\":\n",
    "        raise(\"Failed geometryType check!\")\n",
    "\n",
    "\n",
    "# construct geojson\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"crs\": { \"type\": \"name\", \"properties\": { \"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\" } },\n",
    "    \"features\": []\n",
    "}\n",
    "\n",
    "\n",
    "# add to geojson\n",
    "for d in zone[\"results\"]:\n",
    "    tmp = {\"type\": \"Feature\", \"properties\" : d[\"attributes\"], \"geometry\":{\"type\": None, \"coordinates\": None}}\n",
    "    \n",
    "    if len(d[\"geometry\"][\"rings\"]) == 1 or d[\"value\"] == \"EKPC\": #TODO: hardcoding EKPC for now, which is a donut shape\n",
    "        tmp[\"geometry\"][\"type\"] = \"Polygon\"\n",
    "        geojson[\"features\"].append(tmp)\n",
    "        tmp[\"geometry\"][\"coordinates\"] = d[\"geometry\"][\"rings\"]\n",
    "    else:\n",
    "        tmp[\"geometry\"][\"type\"] = \"MultiPolygon\"\n",
    "        geojson[\"features\"].append(tmp)\n",
    "        tmp[\"geometry\"][\"coordinates\"] = [[x] for x in d[\"geometry\"][\"rings\"]]\n",
    "    \n",
    "\n",
    "# write geojson to disk\n",
    "output = open(\"data/pjm_system_map_export/pjm_zone.geojson\", 'w')\n",
    "json.dump(geojson, output)\n",
    "output.close()\n",
    "print(\"geojson file has been written out to disk!\")\n",
    "\n",
    "# load geojson as a geodataframe\n",
    "pjm_zone = gpd.read_file(\"data/pjm_system_map_export/pjm_zone.geojson\")\n",
    "print(\"geojson file has been read as a geodataframe!\")\n",
    "\n",
    "# replace NULL and None values with NaN\n",
    "pjm_zone = pjm_zone.replace({\"Null\": np.nan, None: np.nan, \"\":np.nan})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched substation to zone based on geoemtry\n",
    "\n",
    "zoneGeometryMap = dict(zip(pjm_zone.PLANNING_ZONE_NAME, pjm_zone.geometry))\n",
    "\n",
    "substation_labels[\"matched_zone\"] = np.nan\n",
    "\n",
    "for index, row in substation_labels.iterrows():\n",
    "    if row[\"NAME\"] is not np.nan:\n",
    "        for zone, geometry in zoneGeometryMap.items():\n",
    "            if row[\"geometry\"].within(geometry):\n",
    "                substation_labels.loc[index, \"matched_zone\"] = zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up weighted levenshtein score\n",
    "from weighted_levenshtein import lev\n",
    "\n",
    "insert_costs = np.full((128,), 1, dtype=np.float64)\n",
    "delete_costs = np.full((128,), 5, dtype=np.float64)\n",
    "substitute_costs = np.full((128, 128), 99999, dtype=np.float64)\n",
    "\n",
    "def weighted_lev(word1, word2):\n",
    "    return lev(word1, word2,\n",
    "               insert_costs=insert_costs,\n",
    "               delete_costs=delete_costs,\n",
    "               substitute_costs=substitute_costs)\n",
    "\n",
    "\n",
    "def find_match(word1, choices):\n",
    "    tmp_word1 = word1.lower()\n",
    "    tmp_choices = list(map(lambda x: x.lower(), choices))\n",
    "    scores = list(map(lambda x: weighted_lev(tmp_word1, x), tmp_choices))\n",
    "    min_index = scores.index(min(scores))\n",
    "    return (choices[min_index], scores[min_index])\n",
    "\n",
    "\n",
    "find_match(\"FELCTY\", ['Taylor', 'Felicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1723\n",
      "1 2971\n",
      "2 3011\n",
      "3 4084\n",
      "4 4216\n",
      "5 4698\n",
      "6 5427\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# matching\n",
    "# as more matches ahppen, gradually decrease in the threshold for fuzzy match\n",
    "# the idea is to match with gradually decreasing confidence\n",
    "\n",
    "# first iteration: perfect match confidence, zone matches or voltage matches\n",
    "# second iteration: high match confidence, zone matches or voltage matches\n",
    "# third iteration: high match confidence\n",
    "# fourth iteration: intermediate match confidence\n",
    "# fifth iteration: remaining substations\n",
    "\n",
    "# revise substation_labels dataframe as needed\n",
    "substation_labels[\"VOLTAGE\"] = substation_labels.VOLTAGE.astype(float)\n",
    "substation_labels = substation_labels.drop_duplicates()\n",
    "substation_labels[\"NAME\"] = substation_labels[\"NAME\"].astype(str).apply(lambda x: re.sub(r'[^\\w]', '', x))\n",
    "\n",
    "# initialize dictionaries and lists\n",
    "yet_to_be_matched_sublabels = substation_labels.SUBSTATION_GLOBALID.to_list()\n",
    "yet_to_be_matched_nodes = node_list.groupby(\"substation\")[\"zone\"].unique().to_dict()\n",
    "mapping = {}\n",
    "\n",
    "\n",
    "# # get pnode and list of voltages using pnode_list and equiplist\n",
    "# node_voltage_map = {}\n",
    "# for x in node_list.substation.unique():\n",
    "#     tmp_voltage = equiplist[equiplist.STATION == x].VOLTAGE.apply(lambda x: float(x.replace(\" KV\", \"\"))).unique()\n",
    "#     node_voltage_map[x] = tmp_voltage\n",
    "\n",
    "\n",
    "matching_iterations = {\n",
    "    0: {\"threshold\": 100, \"zone_check\": True, \"fuzz score\": True, \"description\": \"exact match in same zone\"},\n",
    "    1: {\"threshold\": 90, \"zone_check\": True, \"fuzz score\": True, \"description\": \"high match in same zone\"},\n",
    "    2: {\"threshold\": 95, \"zone_check\": False, \"fuzz score\": True, \"description\": \"high match in all zones\"},\n",
    "    3: {\"threshold\": 5, \"zone_check\": True, \"fuzz score\": False, \"description\": \"medium match in same zone with weighted levenshtein\"},\n",
    "    4: {\"threshold\": 5, \"zone_check\": False, \"fuzz score\": False, \"description\": \"medium match in same zone with weighted levenshtein\"},\n",
    "    5: {\"threshold\": 10, \"zone_check\": True, \"fuzz score\": False, \"description\": \"low match in same zone with weighted levenshtein\"},\n",
    "    6: {\"threshold\": np.inf, \"zone_check\": False, \"fuzz score\": False, \"description\": \"remaining same zone with weighted levenshtein\"}\n",
    "}\n",
    "\n",
    "\n",
    "for index, threshold in enumerate([100, 90, 95, 5, 5, 10, np.inf]):\n",
    "\n",
    "    for name in list(yet_to_be_matched_nodes):\n",
    "        # TODO: need to add condition to skip substations that do not have >= 69 voltages\n",
    "\n",
    "        zone = yet_to_be_matched_nodes[name]\n",
    "        \n",
    "        tmp_sub_labels = substation_labels[substation_labels.SUBSTATION_GLOBALID.isin(yet_to_be_matched_sublabels)]\n",
    "\n",
    "        # TODO: tmp. adding zone check here\n",
    "        if index in [0, 1, 3, 5]:\n",
    "            tmp_sub_labels = tmp_sub_labels[(tmp_sub_labels.PLANNING_ZONE_NAME.isin(zone)) | (tmp_sub_labels.matched_zone.isin(zone))]\n",
    "\n",
    "        # TODO: the fix below is temporary to avoid exception\n",
    "        if tmp_sub_labels.shape[0] == 0:\n",
    "            continue\n",
    "            \n",
    "        # preprocessing\n",
    "        if (\"ComEd\" in zone) and bool(re.match(r\"^\\d+\\s+(\\w+)$\", name)):\n",
    "            # COMED's naming convention makes it difficult, and need to be handled separately\n",
    "            tmp_name = re.search(r\"^\\d+\\s+(\\w+)$\", name).group(1)        \n",
    "        elif ((\"ATSI\" in zone) or (\"DEOK\" in zone) or (\"Dayton\" in zone)) and bool(re.match(r\"^\\d+(\\w+)$\", name)):\n",
    "             # ATSI, DEOK, and Dayton have naming convention that starts with digits, which make matching difficult\n",
    "            tmp_name = re.search(r\"^\\d+(\\w+)$\", name).group(1)\n",
    "        else:\n",
    "            tmp_name = name\n",
    "\n",
    "        choices = tmp_sub_labels[\"NAME\"].dropna().to_list() # TODO: currently dropping nan, considering replacing with \"\", i.e. empty string\n",
    "        \n",
    "        if index < 3:\n",
    "            match = process.extractOne(tmp_name, choices)\n",
    "        else:\n",
    "            tmp_name = tmp_name.replace(\"_\", \"\")\n",
    "            match = find_match(tmp_name, choices)\n",
    "\n",
    "        if index < 3 and match[1] < threshold:\n",
    "            continue\n",
    "        \n",
    "        if index >= 3 and match[1] > threshold:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        # get substation id and name from substation_labels\n",
    "        sub_id = tmp_sub_labels[tmp_sub_labels.NAME == match[0]][\"SUBSTATION_GLOBALID\"].values[0]\n",
    "        sub_name = tmp_sub_labels[tmp_sub_labels.NAME == match[0]][\"NAME\"].values[0]\n",
    "\n",
    "        # add to dictionary\n",
    "        mapping[name] = (sub_name, zone, sub_id, match[1], index)\n",
    "\n",
    "        # remove from list and dicionaries\n",
    "        yet_to_be_matched_sublabels.remove(sub_id)\n",
    "        del yet_to_be_matched_nodes[name]\n",
    "    \n",
    "    # print the iteration number and current number of matching\n",
    "    print(index, len(mapping.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "substation_pnode_match = pd.DataFrame(mapping).T\n",
    "substation_pnode_match.columns = [\"system_map_substation_name\", \"pnode_zone\",\n",
    "                                  \"system_map_substation_id\",\n",
    "                                  \"match_score\", \"match_round\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_map_substation_name</th>\n",
       "      <th>pnode_zone</th>\n",
       "      <th>system_map_substation_id</th>\n",
       "      <th>match_score</th>\n",
       "      <th>match_round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>02ANGOLA</th>\n",
       "      <td>Angola</td>\n",
       "      <td>[ATSI]</td>\n",
       "      <td>{FD0A8435-45DB-4C01-9D2F-571C762672B1}</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02ARMCO</th>\n",
       "      <td>ARMCO</td>\n",
       "      <td>[ATSI]</td>\n",
       "      <td>{D5F74F69-1130-4D5F-BC31-95F75F02C577}</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02BRUSH</th>\n",
       "      <td>Brush</td>\n",
       "      <td>[ATSI]</td>\n",
       "      <td>{424EF662-40EB-46E2-B0C3-9EE5BA8446D1}</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02DARWIN</th>\n",
       "      <td>Darwin</td>\n",
       "      <td>[ATSI]</td>\n",
       "      <td>{C157ABB7-B65E-4DD0-8FE7-C3C4BBE1A6C9}</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02DAWSON</th>\n",
       "      <td>Dawson</td>\n",
       "      <td>[ATSI]</td>\n",
       "      <td>{33778DA1-A58B-4EBB-8D0E-BCD20BF30B59}</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWATARA</th>\n",
       "      <td>Swatara</td>\n",
       "      <td>[PPL]</td>\n",
       "      <td>{2ED6B6C4-CB5E-40D4-8E67-A9029D7721CF}</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THELMAEK</th>\n",
       "      <td>ThelmaEK</td>\n",
       "      <td>[EKPC]</td>\n",
       "      <td>{C379C87D-B040-4494-B7EC-E0FFC10E16E7}</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USAP</th>\n",
       "      <td>USAP</td>\n",
       "      <td>[DL]</td>\n",
       "      <td>{7DE0FF6C-55B8-402F-95AC-8D038A74C996}</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USGYPSUM</th>\n",
       "      <td>USGypsum</td>\n",
       "      <td>[AEP]</td>\n",
       "      <td>{9B165497-77DF-416A-A7E4-8AFB00B6BA0E}</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WESLEY</th>\n",
       "      <td>Wesley</td>\n",
       "      <td>[AEP]</td>\n",
       "      <td>{5E8C3E3A-A9CC-4219-A6B4-096DD0B7BBC8}</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3011 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         system_map_substation_name pnode_zone  \\\n",
       "02ANGOLA                     Angola     [ATSI]   \n",
       "02ARMCO                       ARMCO     [ATSI]   \n",
       "02BRUSH                       Brush     [ATSI]   \n",
       "02DARWIN                     Darwin     [ATSI]   \n",
       "02DAWSON                     Dawson     [ATSI]   \n",
       "...                             ...        ...   \n",
       "SWATARA                     Swatara      [PPL]   \n",
       "THELMAEK                   ThelmaEK     [EKPC]   \n",
       "USAP                           USAP       [DL]   \n",
       "USGYPSUM                   USGypsum      [AEP]   \n",
       "WESLEY                       Wesley      [AEP]   \n",
       "\n",
       "                        system_map_substation_id match_score match_round  \n",
       "02ANGOLA  {FD0A8435-45DB-4C01-9D2F-571C762672B1}         100           0  \n",
       "02ARMCO   {D5F74F69-1130-4D5F-BC31-95F75F02C577}         100           0  \n",
       "02BRUSH   {424EF662-40EB-46E2-B0C3-9EE5BA8446D1}         100           0  \n",
       "02DARWIN  {C157ABB7-B65E-4DD0-8FE7-C3C4BBE1A6C9}         100           0  \n",
       "02DAWSON  {33778DA1-A58B-4EBB-8D0E-BCD20BF30B59}         100           0  \n",
       "...                                          ...         ...         ...  \n",
       "SWATARA   {2ED6B6C4-CB5E-40D4-8E67-A9029D7721CF}         100           2  \n",
       "THELMAEK  {C379C87D-B040-4494-B7EC-E0FFC10E16E7}         100           2  \n",
       "USAP      {7DE0FF6C-55B8-402F-95AC-8D038A74C996}         100           2  \n",
       "USGYPSUM  {9B165497-77DF-416A-A7E4-8AFB00B6BA0E}         100           2  \n",
       "WESLEY    {5E8C3E3A-A9CC-4219-A6B4-096DD0B7BBC8}         100           2  \n",
       "\n",
       "[3011 rows x 5 columns]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: currently using only the most truthworthy match, which are from the first two rounds\n",
    "\n",
    "substation_pnode_match[substation_pnode_match.match_round.isin([0, 1, 2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map System Map Lines to Line Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge line_rating with equiplist\n",
    "# TODO: currently averaging line rating over different conditions, consider improving this in the future\n",
    "\n",
    "# load equipment list\n",
    "equiplist = pd.read_csv(\"data/oasis/equiplist.csv\", skiprows=1)\n",
    "equiplist[\"VOLTAGE\"] = equiplist[\"VOLTAGE\"].apply(lambda x: float(x.replace(\"KV\", \"\")))\n",
    "\n",
    "# merge equiplist with line rating\n",
    "# TODO: currently disabling line rating\n",
    "# line_rating = line_rating.groupby([\"company\", \"substation\", \"voltage\", \"device\", \"end\", \"description\"]).mean()\n",
    "# line_rating = line_rating[[\"day_normal\"]]\n",
    "# equiplist = pd.merge(equiplist, line_rating, left_on=\"LONG NAME\", right_on=\"description\", how=\"left\")\n",
    "line_equiplist = equiplist[equiplist.TYPE == \"LINE\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CABINCRE-HERNSHAW CAB-HER\n",
      "FIELDALE-SHEFFIEL\n",
      "KAMMER-MUSKINGU 345KV\n",
      "DELAWAIM-SELMAPAR\n",
      "ASTOR   -REFUGEE  AST-REF\n",
      "HILSBORO-MIDD DAY\n",
      "HYATTCS  1A       XFORMER\n",
      "HYATTCS  1B       XFORMER\n",
      "MILTONSW-LICK     MIL-LIC1\n",
      "NHAVERHI NHAV XF  XFORMER\n",
      "CORDERCR-PRUNTYTO 12\n",
      "GORMANIFOLD GN-CE\n",
      "EMPORADP-SKIPPERS 1029A\n",
      "TREGO-TREGOTAP 130B1TMP\n",
      "BLMNTDOM-BRAMBLET 206A\n",
      "CHOWAN  -SHERTFOR 2131C\n",
      "CLAYVILL-NWCST_EK CLA-NWCS\n",
      "SUMSHADT-SUMMER   TIE\n",
      "INLAND S556_TIE\n",
      "INLAND S567_TIE\n",
      "INLAND S578_TIE\n",
      "INLAND S581_TIE\n",
      "OYSTERCR NC   CB\n",
      "OYSTERCR NS   CB\n",
      "SMITHBUR EB   CB\n",
      "HOMERCIT 304          CB\n",
      "HOMERCIT 307          CB\n",
      "HOMERCIT 308          CB\n",
      "MERCER   220-1    GSU XFMR\n",
      "MERCER   220-2    GSU XFMR\n"
     ]
    }
   ],
   "source": [
    "# check if line rating and equiplist are easy to match with each other\n",
    "# answer is yes\n",
    "# as seen below, most that are in line rating can be directly matched with equiplist\n",
    "\n",
    "line_rating = pd.read_csv(\"data/oasis/line_rating.csv\")\n",
    "\n",
    "for x in line_rating.description.unique():\n",
    "    if x not in equiplist[\"LONG NAME\"].to_list():\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(zip(substation_labels[\"SUBSTATION_GLOBALID\"], tuple(zip(substation_labels[\"NAME\"], substation_labels[\"PLANNING_ZONE_NAME\"]))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge lines and substations\n",
    "lines_tmp = lines[[\"SUBSTATION_A_GLOBALID\", \"SUBSTATION_B_GLOBALID\", \"TO_LINE_NAME\", \"VOLTAGE\"]]\n",
    "# sub_tmp = substation_labels[[\"NAME\", \"SUBSTATION_GLOBALID\", \"VOLTAGE\", \"PLANNING_ZONE_NAME\"]]\n",
    "sub_tmp = substation_labels[[\"NAME\", \"SUBSTATION_GLOBALID\"]]\n",
    "\n",
    "\n",
    "line_sub = pd.merge(lines_tmp, sub_tmp, how=\"left\",\n",
    "                    left_on=\"SUBSTATION_A_GLOBALID\", right_on=\"SUBSTATION_GLOBALID\",\n",
    "                    suffixes=(\"_LINE\", \"_SUBSTATION_A\"))\n",
    "\n",
    "line_sub = pd.merge(line_sub, sub_tmp, how=\"left\",\n",
    "                    left_on=\"SUBSTATION_B_GLOBALID\", right_on=\"SUBSTATION_GLOBALID\",\n",
    "                    suffixes=(\"_SUBSTATION_A\", \"_SUBSTATION_B\"))\n",
    "\n",
    "line_sub = line_sub.drop(columns=[\"SUBSTATION_GLOBALID_SUBSTATION_A\", \"SUBSTATION_GLOBALID_SUBSTATION_B\"])\n",
    "\n",
    "# line_sub.LINE_NAME = line_sub.LINE_NAME.apply(lambda x: x.strip() if (x is not np.nan) else x)\n",
    "# line_sub.LINE_NAME_ALT = line_sub.LINE_NAME_ALT.apply(lambda x: x.strip() if (x is not np.nan) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for the lines equipment list, separate and obtain the information on the two connected substations\n",
    "\n",
    "# # add column for subA, subB\n",
    "# line_equiplist[\"subA\"] = np.nan\n",
    "# line_equiplist[\"subB\"] = np.nan\n",
    "\n",
    "# equiplist_sub_list = list(equiplist.STATION.unique())\n",
    "# # equiplist_sub_list = list(map(lambda x: re.sub(r'\\s+', ' ', x), equiplist_sub_list))\n",
    "\n",
    "# # find subA and subB for line\n",
    "# # TODO: currently some substations are missing, come back to fix it by adding more logic to parsing\n",
    "# for index, row in line_equiplist.iterrows():\n",
    "#     subA = row[\"STATION\"]\n",
    "#     longName = row[\"LONG NAME\"]\n",
    "    \n",
    "#     longName = re.sub(r'\\s+-', '-', longName) # experimenting, eliminating white space before -\n",
    "    \n",
    "#     found = False\n",
    "    \n",
    "#     # subA-subB somethingsomething\n",
    "#     for subB in equiplist_sub_list:\n",
    "#         longName_tmp = \"-\".join([subA, subB]) + \" \"\n",
    "#         if longName_tmp in longName:\n",
    "#             found = True\n",
    "#             line_equiplist.loc[index, \"subA\"] = subA\n",
    "#             line_equiplist.loc[index, \"subB\"] = subB\n",
    "#             break\n",
    "    \n",
    "#     # subA-subB\n",
    "#     if not found:\n",
    "#         for subB in equiplist_sub_list:\n",
    "#             longName_tmp = \"-\".join([subA, subB])\n",
    "#             if longName_tmp == longName:\n",
    "#                 found = True\n",
    "#                 line_equiplist.loc[index, \"subA\"] = subA\n",
    "#                 line_equiplist.loc[index, \"subB\"] = subB\n",
    "#                 break\n",
    "    \n",
    "#     # subB\\s or subB\n",
    "#     if not found:\n",
    "#         for subB in equiplist_sub_list:\n",
    "#             if (subB + \" \") in longName or bool(re.search(r\"{}$\".format(subB), longName)):\n",
    "#                 found = True\n",
    "#                 line_equiplist.loc[index, \"subA\"] = subA\n",
    "#                 line_equiplist.loc[index, \"subB\"] = subB\n",
    "#                 break\n",
    "    \n",
    "#     # ends in subB\\d\n",
    "#     if not found:\n",
    "#         for subB in equiplist_sub_list:\n",
    "#             if bool(re.search(r\"{}\\d($|\\s)\".format(subB), longName)):\n",
    "#                 found = True\n",
    "#                 line_equiplist.loc[index, \"subA\"] = subA\n",
    "#                 line_equiplist.loc[index, \"subB\"] = subB\n",
    "#                 break\n",
    "\n",
    "# line_equiplist.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get substation, company zone mapping information into a single dataframe\n",
    "station_company_zone_map = pd.concat([equiplist.groupby(\"STATION\")[\"COMPANY\"].unique(),\n",
    "                                      equiplist.groupby(\"STATION\")[\"ZONE\"].unique()], axis=1)\n",
    "\n",
    "if station_company_zone_map.COMPANY.apply(lambda x: len(x)).sum() != station_company_zone_map.shape[0]:\n",
    "    raise\n",
    "\n",
    "station_company_zone_map.COMPANY = station_company_zone_map.COMPANY.apply(lambda x: x[0])\n",
    "\n",
    "line_equiplist = pd.merge(line_equiplist,\n",
    "                          station_company_zone_map.rename(columns={\"COMPANY\": \"subA_COMPANY\", \"ZONE\": \"subA_ZONE\"}),\n",
    "                          left_on=\"subA\", right_on=\"STATION\", how=\"left\")\n",
    "\n",
    "line_equiplist = pd.merge(line_equiplist,\n",
    "                          station_company_zone_map.rename(columns={\"COMPANY\": \"subB_COMPANY\", \"ZONE\": \"subB_ZONE\"}),\n",
    "                          left_on=\"subB\", right_on=\"STATION\", how=\"left\")\n",
    "\n",
    "# # convert line voltage to float\n",
    "# line_equiplist.VOLTAGE = line_equiplist.VOLTAGE.apply(lambda x: float(x.replace(\"KV\", \"\")))\n",
    "# line_sub.LINE_VOLTAGE = line_sub.LINE_VOLTAGE.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # match the line equipment list with pjm system map substation names\n",
    "# # voltage level by voltage level\n",
    "# line_sub[\"SUB_A_MATCH\"] = line_sub[\"SUB_A_ID\"]\n",
    "# line_sub[\"SUB_B_MATCH\"] = line_sub[\"SUB_B_ID\"]\n",
    "\n",
    "# # iterate over voltage\n",
    "# for voltage_level in [765]: #sorted(lines.VOLTAGE.unique(), reverse=True):    \n",
    "#     # get substations in system map to be matched\n",
    "#     tmp = line_sub[line_sub.LINE_VOLTAGE == voltage_level].dropna(subset=[\"SUB_A_NAME\", \"SUB_B_NAME\"]) # TODO: currently dropping nans, shouldn't be dropping anything in the future\n",
    "#     sub_match = {**dict(zip(tmp.SUB_A_ID, tmp.SUB_A_NAME)), **dict(zip(tmp.SUB_B_ID, tmp.SUB_B_NAME))}\n",
    "    \n",
    "#     # get a list of choices\n",
    "#     choices = list(line_equiplist[line_equiplist.VOLTAGE == voltage_level].subA.dropna().unique()) # TODO: currently dropping nans, shouldn't be dropping anything in the future\n",
    "#     choices+= list(line_equiplist[line_equiplist.VOLTAGE == voltage_level].subB.dropna().unique()) # TODO: currently dropping nans, shouldn't be dropping anything in the future\n",
    "    \n",
    "#     for s in sub_match.keys():\n",
    "#         sub_name = sub_match[s]\n",
    "#         match = process.extractOne(sub_name, choices)\n",
    "#         sub_match[s] = match[0]\n",
    "    \n",
    "#     # TODO: add check based on fuzzy match score\n",
    "#     # TODO: add check that the matched substations form a line in system map\n",
    "#     # TODO: add check that the matched substations are from the expected tranmission zone\n",
    "    \n",
    "#     # replace\n",
    "#     line_sub[\"SUB_A_MATCH\"] = line_sub[\"SUB_A_MATCH\"].replace(sub_match)\n",
    "#     line_sub[\"SUB_B_MATCH\"] = line_sub[\"SUB_B_MATCH\"].replace(sub_match)\n",
    "\n",
    "\n",
    "# # # deal with left over substations\n",
    "# # # TODO: this step is extrafluous, should eliminate in the future in favor of a improved step above\n",
    "# # for index, row in line_sub.iterrows():\n",
    "# #     if row[\"SUB_A_MATCH\"] == row[\"SUB_A_ID\"]:\n",
    "# #         line_sub.loc[index, \"SUB_A_MATCH\"] = np.nan\n",
    "# #     if row[\"SUB_B_MATCH\"] == row[\"SUB_B_ID\"]:\n",
    "# #         line_sub.loc[index, \"SUB_B_MATCH\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = line_equiplist[line_equiplist.VOLTAGE == \"765 KV\"][\"LONG NAME\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maliszewski Marysville ----- MALISZEW-MARYSVI2 MAL-MAR1 ----- 67\n",
      "Culloden Gavin ----- CULLODE2-GAVINAEP ----- 84\n",
      "Kammer SouthCanton ----- KAMMER2-SCANTON2 ----- 82\n",
      "Jefferson Rockport ----- JEFFERSO-ROCKPOR2 ----- 91\n",
      "Rockport RockportWorks ----- ROCKPOR2-ROCKPWKS2 ----- 80\n",
      "Cloverdale JoshuaFalls ----- CLOVERD2-JOSHUAF2 ----- 77\n",
      "Belmont Mountaineer ----- AMOS-MOUNTAIN ----- 69\n",
      "Greentown Reynolds ----- GREENTO2-REYNOLD2 GRE-REY ----- 70\n",
      "NorthProctorville HangingRock ----- NPROCTO2-HANGING2 ----- 65\n",
      "Culloden Baker ----- CULLODE2-BAKER ----- 93\n",
      "Marysville Flatlick ----- MARYSVI2-FLATLICK ----- 89\n",
      "Axton JacksonsFerry ----- AXTONAEP-JACKSONS ----- 78\n",
      "HangingRock DonMarquis ----- CORNU-HANGING ROCK ----- 75\n",
      "JohnAmos Mountaineer ----- AMOS-MOUNTAIN ----- 79\n",
      "Dumont Sorenson ----- DUMONT2-SORENSON DUM-SOR ----- 77\n",
      "Culloden Wyoming ----- CULLODE2-WYOMING2 ----- 91\n",
      "Maliszewski Vassell ----- MALISZEW-VASSELL  MAL-VAS ----- 79\n",
      "Dumont WiltonCtr ----- DUMONT2-WILTON ----- 87\n",
      "CornuHangingRock HangingRock ----- CORNU-HANGING ROCK ----- 74\n",
      "Belmont Kammer ----- BELMONT-KAMMER ----- 100\n",
      "Gavin Flatlick ----- GAVINAEP-FLATLICK ----- 90\n",
      "JohnAmos NorthProctorville ----- AMOS-NPROCTO2 ----- 62\n",
      "Jefferson Greentown ----- JEFFERSO-GREENTO2 TIE ----- 80\n",
      "Mountaineer Gavin ----- MOUNTAIN-GAVINAEP ----- 82\n",
      "Cloverdale JacksonsFerry ----- CLOVERD2-JACKSONS ----- 78\n",
      "Collins Plano ----- 23 COLLI-167 PLAN 2315 ----- 57\n",
      "HangingRock Jefferson ----- HANGING2-JEFFERSO ----- 84\n",
      "Baker Broadford ----- BAKER-BROADFO2 ----- 90\n",
      "Baker HangingRock ----- BAKER-HANGING2 ----- 84\n",
      "Rockport Sullivan ----- ROCKPOR2-SULLIVA2 ----- 88\n",
      "Dumont Greentown ----- DUMONT2-GREENTO2 TIE ----- 78\n",
      "Kammer Vassell ----- KAMMER2 -VASSELL  KAM-VAS ----- 76\n",
      "Rockport RockportWorks ----- ROCKPOR2-ROCKPWKS2 ----- 80\n",
      "DonaldCCook Dumont ----- COOK-DUMONT2 ----- 73\n",
      "JacksonsFerry Wyoming ----- JACKSONS-WYOMING2 JAC-WYO1 ----- 72\n",
      "JohnAmos Culloden ----- CULLODE2-GAVINAEP ----- 59\n",
      "Collins WiltonCtr ----- DUMONT2-WILTON ----- 58\n",
      "Sorenson Marysville ----- MARYSVI2-SORENSON MAR-SOR ----- 73\n",
      "Broadford JacksonsFerry ----- BROADFO2-JACKSONS ----- 80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize dictionaries and lists\n",
    "mapping = {}\n",
    "\n",
    "\n",
    "for index, row in line_sub[line_sub.VOLTAGE == \"765\"].iterrows():\n",
    "    \n",
    "    yet_to_be_matched_map_lines = substation_labels.SUBSTATION_GLOBALID.to_list()\n",
    "    yet_to_be_matched_equiplist_lines = node_list.groupby(\"substation\")[\"zone\"].unique().to_dict()\n",
    "\n",
    "    \n",
    "    query = row[\"NAME_SUBSTATION_A\"] + \" \" + row[\"NAME_SUBSTATION_B\"]\n",
    "    match = process.extractOne(query, choices, scorer=fuzz.token_sort_ratio)\n",
    "    \n",
    "    print(query, \"-----\", match[0], \"-----\", match[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMOS-CULLODE2',\n",
       " 'AMOS-MOUNTAIN',\n",
       " 'AMOS-NPROCTO2',\n",
       " 'AXTONAEP-JACKSONS',\n",
       " 'BROADFO2-JACKSONS',\n",
       " 'CLOVERD2-JACKSONS',\n",
       " 'CLOVERD2-JOSHUAF2',\n",
       " 'CULLODE2-BAKER',\n",
       " 'CULLODE2-GAVINAEP',\n",
       " 'CULLODE2-WYOMING2',\n",
       " 'JACKSONS-WYOMING2 JAC-WYO1',\n",
       " 'KAMMER2-SCANTON2',\n",
       " 'KAMMER2 -VASSELL  KAM-VAS',\n",
       " 'MOUNTAIN-GAVINAEP',\n",
       " 'COOK-DUMONT2',\n",
       " 'DUMONT2-GREENTO2 TIE',\n",
       " 'DUMONT2-SORENSON DUM-SOR',\n",
       " 'DUMONT2-WILTON',\n",
       " 'JEFFERSO-GREENTO2 TIE',\n",
       " 'JEFFERSO-ROCKPOR2',\n",
       " 'ROCKPOR2-ROCKPWKS2',\n",
       " 'ROCKPOR2-ROCKPWKS3',\n",
       " 'ROCKPOR2-SULLIVA2',\n",
       " 'BAKER-BROADFO2',\n",
       " 'BAKER-HANGING2',\n",
       " 'CORNU-HANGING ROCK',\n",
       " 'GAVINAEP-FLATLICK',\n",
       " 'HANGING2-JEFFERSO',\n",
       " 'HANGING2-MARQUIS2',\n",
       " 'MALISZEW-MARYSVI2 MAL-MAR1',\n",
       " 'MALISZEW-VASSELL  MAL-VAS',\n",
       " 'MARYSVI2-FLATLICK',\n",
       " 'MARYSVI2-SORENSON MAR-SOR',\n",
       " 'NPROCTO2-HANGING2',\n",
       " 'BELMONT-KAMMER',\n",
       " 'BELMONT-MOUN AEP',\n",
       " 'GREENTO2-REYNOLD2 GRE-REY',\n",
       " '23 COLLI-112 WILT 11216',\n",
       " '23 COLLI-167 PLAN 2315',\n",
       " 'CHATEAUG-MASSENA',\n",
       " 'MARCY-MASSENA MSU-1']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replace zonal name to accomodate matching with equiplist\n",
    "# zonalMap = {'AECO':'AE', 'ATSI':'FE', 'BGE':'BC', 'DAY':'DAYTON', 'DUQ':'DUQU', 'JCPL':'JC',\n",
    "#             'METED':'ME', 'PECO':'PE', 'PENELEC':'PN', 'PEPCO':'PEP', 'PPL':'PL', 'PSEG':'PS'}\n",
    "\n",
    "# node_list.zone = node_list.zone.replace(zonalMap)\n",
    "\n",
    "# # check that all zones are node list can be found in equiplist\n",
    "# for x in node_list.zone.unique():\n",
    "#     if x not in equiplist.COMPANY.unique():\n",
    "#         print(x)\n",
    "        \n",
    "\n",
    "# pnode_list_sub_zone_mapping = node_list.groupby(\"substation\")[\"zone\"].unique().apply(lambda x: list(x)).to_dict()\n",
    "# equiplist_node_pnode_map = {}\n",
    "\n",
    "# # match pnode list with equipment list\n",
    "# for query in pnode_list_sub_zone_mapping.keys():\n",
    "#     sub_zone = pnode_list_sub_zone_mapping[query]\n",
    "#     choices = list(line_equiplist[line_equiplist.COMPANY.isin(sub_zone)].subA.unique()) #TODO: should it be line_equiplist or equiplist?\n",
    "#     choices+= list(line_equiplist[line_equiplist.COMPANY.isin(sub_zone)].subB.unique()) #TODO: should it be line_equiplist or equiplist?\n",
    "#     match = process.extractOne(str(query), choices) #TODO: why do we need str() here?\n",
    "#     equiplist_node_pnode_map[match[0]] = str(query) #TODO: why do we need str() here?\n",
    "    \n",
    "#     # TODO: maybe consider do exact match rather than fuzzy match for better precision\n",
    "#     # TODO; then use zone as a check\n",
    "    \n",
    "#     # TODO: QA/QC on the matching - should be better\n",
    "#     # TODO: add check based on fuzzy match score\n",
    "#     # TODO: add check based on voltage\n",
    "#     # TODO: check matching is one-to-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # map line_sub to pnode\n",
    "\n",
    "# line_sub[\"SUB_A_PNODE\"] = np.nan\n",
    "# line_sub[\"SUB_B_PNODE\"] = np.nan\n",
    "\n",
    "# for index, row in line_sub.iterrows():\n",
    "#     subAMatch = row[\"SUB_A_MATCH\"]\n",
    "#     subBMatch = row[\"SUB_B_MATCH\"]\n",
    "    \n",
    "#     if subAMatch in equiplist_node_pnode_map.keys():\n",
    "#         line_sub.loc[index, \"SUB_A_PNODE\"] = equiplist_node_pnode_map[subAMatch]\n",
    "#     if subBMatch in equiplist_node_pnode_map.keys():\n",
    "#         line_sub.loc[index, \"SUB_B_PNODE\"] = equiplist_node_pnode_map[subBMatch]\n",
    "\n",
    "# # TODO: need to do QA/QC on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use contingency list to find missing substations\n",
    "\n",
    "# # load contingency list\n",
    "# with open('data/oasis/contingency_list.json') as f:\n",
    "#     contingency_list = json.load(f)[\"Contingencies\"][\"Contingency\"]\n",
    "\n",
    "# contingency_list = {x[\"Name\"]: [y[\"_b1\"] for y in x[\"Equipment\"][\"ContingencyEquipment\"]] for x in contingency_list}\n",
    "\n",
    "# for index, row in line_equiplist[line_equiplist.subA.isnull()].iterrows():\n",
    "#     subA = row[\"STATION\"]\n",
    "#     longName = row[\"LONG NAME\"]\n",
    "#     voltage = str(int(row[\"VOLTAGE\"]))\n",
    "    \n",
    "#     matchedGroups = re.search(r\"^([\\w\\s.]+)-\\s?([\\w.]+)\", longName)\n",
    "    \n",
    "#     subB = matchedGroups.group(2)\n",
    "\n",
    "# #     print(longName)\n",
    "#     for contingency in contingency_list.keys():\n",
    "#         tmp = contingency.lower()\n",
    "#         if subB.lower() in tmp:\n",
    "#             match = process.extractOne(subB, contingency_list[contingency])\n",
    "#             print(longName, \"---\", subB, match[0], match[1])\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 500 kv, skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA/QC\n",
    "# are all lines in equiplist present in pjm system map?\n",
    "# are all lines in pjm system map present in equiplist?\n",
    "\n",
    "# after mapping, manual review those match with low fuzzy match/levenshtein score\n",
    "\n",
    "# use existing zipcode mapping to do QA/QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Generation (Planning Queue & EIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load planning queue data\n",
    "\n",
    "# load data\n",
    "with open('data/pjm_system_map_export/queue.json') as f:\n",
    "    data = json.load(f)\n",
    "print(\"number of data points:\", len(data[\"results\"]))\n",
    "\n",
    "\n",
    "# check data\n",
    "for x in data[\"results\"]:\n",
    "    if x[\"layerId\"] != 0:\n",
    "        raise(\"Failed layerId check!\")\n",
    "    if x[\"layerName\"] != \"Queues\":\n",
    "        raise(\"Failed layerName check!\")\n",
    "    if x[\"geometryType\"] != \"esriGeometryPoint\":\n",
    "        raise(\"Failed geometryType check!\")\n",
    "print(\"passed data check\")\n",
    "\n",
    "\n",
    "# construct geojson\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"crs\": { \"type\": \"name\", \"properties\": { \"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\" } }, # this is not the appropriate projection\n",
    "    \"features\": [\n",
    "    {\n",
    "        \"type\": \"Feature\",\n",
    "        \"properties\" : d[\"attributes\"],\n",
    "        \"geometry\" : {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [d[\"geometry\"][\"x\"], d[\"geometry\"][\"y\"]],\n",
    "            }\n",
    "     } for d in data[\"results\"]]\n",
    "}\n",
    "\n",
    "print(\"finished constructing geojson\")\n",
    "\n",
    "\n",
    "# write geojson to disk\n",
    "output = open(\"data/pjm_system_map_export/queue.geojson\", 'w')\n",
    "json.dump(geojson, output)\n",
    "output.close()\n",
    "print(\"geojson file has been written out to disk!\")\n",
    "\n",
    "# load geojson as a geodataframe\n",
    "queue = gpd.read_file(\"data/pjm_system_map_export/queue.geojson\")\n",
    "print(\"geojson file has been read as a geodataframe!\")\n",
    "\n",
    "# replace NULL and None values with NaN\n",
    "queue = queue.replace({\"Null\": np.nan, None: np.nan, \"\":np.nan})\n",
    "\n",
    "# load queue information exported from PJM as excel\n",
    "queue_info = pd.read_excel(\"data/PlanningQueues_05_25_2020.xlsx\")\n",
    "queue_merged = pd.merge(queue, queue_info, how=\"left\", left_on=\"QUEUE_ID\", right_on=\"Queue Number\")\n",
    "columns = [\"QUEUE_ID\", \"Name\", \"MFO\", \"MW Energy\", \"MW Capacity\", \"VOLTAGE\",\n",
    "           \"MW In Service\", \"Project Type\", \"Fuel\", \"Status\", \"Revised In Service Date\",\n",
    "           \"Actual In Service Date\", \"geometry\"]\n",
    "queue_merged = queue_merged[columns]\n",
    "queue_merged = queue_merged[queue_merged[\"Status\"] == \"In Service\"]\n",
    "print(\"finished merging queue geojson and queue planning info\")\n",
    "print(\"number of data points:\", queue_merged.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_merged.geometry.plot(figsize=(25, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using EIA 860 to map generators to node and coordinates\n",
    "# currently using the 2018 data\n",
    "\n",
    "# load data\n",
    "eia_860_plant = pd.read_excel(\"data/eia8602018/2___Plant_Y2018.xlsx\", skiprows=1)\n",
    "eia_860_gens = pd.read_excel(\"data/eia8602018/3_1_Generator_Y2018.xlsx\", skiprows=1).iloc[:-1, :]\n",
    "eia_860_plant[\"Plant Code\"] = eia_860_plant[\"Plant Code\"].astype(int)\n",
    "eia_860_gens[\"Plant Code\"] = eia_860_gens[\"Plant Code\"].astype(int)\n",
    "\n",
    "\n",
    "# # select relevant columns\n",
    "eia_860_gens = eia_860_gens[[\"Plant Code\", \"Generator ID\", \"Unit Code\", \"Technology\", \"Prime Mover\",\n",
    "                             \"RTO/ISO LMP Node Designation\",\n",
    "                             \"RTO/ISO Location Designation for Reporting Wholesale Sales Data to FERC\"]]\n",
    "\n",
    "eia_860_plant = eia_860_plant[[\"Plant Code\", \"Plant Name\", \"Street Address\", \"City\", \"County\", \"State\",\n",
    "                               \"Latitude\", \"Longitude\", \"Balancing Authority Name\",\n",
    "                               \"Transmission or Distribution System Owner\"]]\n",
    "\n",
    "# check overlapping values\n",
    "# how many plants that are not in gens?\n",
    "plant_code_not_found = []\n",
    "for x in eia_860_plant[\"Plant Code\"].unique():\n",
    "    if x not in eia_860_gens[\"Plant Code\"].unique():\n",
    "        plant_code_not_found.append(x)\n",
    "print(\"in '2___Plant' but no in '3_1_Generator':\", len(plant_code_not_found))\n",
    "\n",
    "# how many in gens that are not in plant?\n",
    "plant_code_not_found = []\n",
    "for x in eia_860_gens[\"Plant Code\"].unique():\n",
    "    if x not in eia_860_plant[\"Plant Code\"].unique():\n",
    "        plant_code_not_found.append(x)\n",
    "print(\"in '3_1_Generator' but not in '2___Plant':\", len(plant_code_not_found))\n",
    "\n",
    "# merge dataframe and select PJM\n",
    "plants = pd.merge(eia_860_plant, eia_860_gens, on=\"Plant Code\", how=\"outer\")\n",
    "plants[\"Plant Code\"] = plants[\"Plant Code\"].astype(str)\n",
    "# plants = plants[plants[\"Balancing Authority Name\"] == \"PJM Interconnection, LLC\"]\n",
    "\n",
    "# eliminate rows that do not have coordinates\n",
    "# TODO: figure out if there are other ways to get coordinates\n",
    "plants = plants[(plants.Longitude != \" \") & (plants.Latitude != \" \")]\n",
    "\n",
    "# construct geodataframe\n",
    "plants = gpd.GeoDataFrame(plants, geometry=gpd.points_from_xy(plants.Longitude, plants.Latitude))\n",
    "\n",
    "# convert projection\n",
    "plants.crs = 4326\n",
    "plants = plants.to_crs(epsg = 3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in queue_merged.iterrows():\n",
    "#     queue_id = row[\"QUEUE_ID\"]\n",
    "#     queue_geo = row[\"geometry\"]\n",
    "    \n",
    "#     min_distance = plants.geometry.apply(lambda x: x.distance(queue_geo)).min()\n",
    "    \n",
    "#     if min_distance < 1000:\n",
    "#         print(queue_id, min_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import nearest_points\n",
    "\n",
    "# unary union of the gpd2 geomtries \n",
    "gpd1 = plants\n",
    "gpd2 = queue_merged\n",
    "\n",
    "pts3 = gpd2.geometry.unary_union\n",
    "def near(point, pts=pts3):\n",
    "    nearest = gpd2.geometry == nearest_points(point, pts)[1]\n",
    "    return gpd2[nearest].QUEUE_ID.values[0]\n",
    "\n",
    "gpd1['Nearest'] = None\n",
    "\n",
    "for index, row in gpd1.iterrows():\n",
    "    try:\n",
    "        gpd1.loc[index, 'Nearest'] = near(row[\"geometry\"])\n",
    "    except:\n",
    "        print(\"Problemaitc geometry:\", index, row[\"Plant Code\"])\n",
    "\n",
    "# gpd1.apply(lambda row: near(row.geometry), axis=1)\n",
    "# gpd1.apply(lambda row: near(row.geometry), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.merge(gpd1, gpd2, left_on=\"Nearest\", right_on=\"QUEUE_ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[tmp[\"Balancing Authority Name\"] == \"PJM Interconnection, LLC\"].to_csv('tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get substation in lines data\n",
    "\n",
    "# ax = states.plot(figsize=(30, 10))\n",
    "# pjm_zone.geometry.plot(ax=ax, color=\"green\")\n",
    "# lines.geometry.plot(ax=ax, color=\"yellow\")\n",
    "# substations.geometry.plot(ax=ax, color=\"red\")\n",
    "# # substations[~substations.PLANNING_ZONE_NAME.isnull()].geometry.plot(ax=ax, color=\"red\")\n",
    "\n",
    "# plants[plants[\"Balancing Authority Name\"] == \"PJM Interconnection, LLC\"].geometry.plot(ax=ax, color=\"black\")\n",
    "\n",
    "# queue_merged.geometry.plot(ax=ax, color=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plants[plants[\"Balancing Authority Name\"] == \"PJM Interconnection, LLC\"][\"Plant Code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjm_zone.geometry.apply(lambda x: x.is_valid)\n",
    "\n",
    "#TODO: APS is still false for some reason. May be because it's a combination of donut and multipolygon shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert state boundaries\n",
    "\n",
    "# load state\n",
    "with open('data/pjm_system_map_export/states.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(\"number of states:\", len(data[\"results\"]))\n",
    "\n",
    "\n",
    "# check data\n",
    "for x in data[\"results\"]:\n",
    "    if x[\"layerId\"] != 18:\n",
    "        raise(\"Failed layerId check!\")\n",
    "    if x[\"layerName\"] != \"States\":\n",
    "        raise(\"Failed layerName check!\")\n",
    "    if x[\"geometryType\"] != \"esriGeometryPolygon\":\n",
    "        raise(\"Failed geometryType check!\")\n",
    "\n",
    "\n",
    "# construct geojson\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"crs\": { \"type\": \"name\", \"properties\": { \"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\" } },\n",
    "    \"features\": []\n",
    "}\n",
    "\n",
    "\n",
    "# add to geojson\n",
    "for d in data[\"results\"]:\n",
    "    tmp = {\"type\": \"Feature\", \"properties\" : d[\"attributes\"], \"geometry\":{\"type\": None, \"coordinates\": None}}\n",
    "    \n",
    "    if len(d[\"geometry\"][\"rings\"]) == 1:\n",
    "        tmp[\"geometry\"][\"type\"] = \"Polygon\"\n",
    "        geojson[\"features\"].append(tmp)\n",
    "        tmp[\"geometry\"][\"coordinates\"] = d[\"geometry\"][\"rings\"]\n",
    "    else:\n",
    "        tmp[\"geometry\"][\"type\"] = \"MultiPolygon\"\n",
    "        geojson[\"features\"].append(tmp)\n",
    "        tmp[\"geometry\"][\"coordinates\"] = [[x] for x in d[\"geometry\"][\"rings\"]]\n",
    "    \n",
    "\n",
    "# write geojson to disk\n",
    "output = open(\"data/pjm_system_map_export/states.geojson\", 'w')\n",
    "json.dump(geojson, output)\n",
    "output.close()\n",
    "print(\"geojson file has been written out to disk!\")\n",
    "\n",
    "# load geojson as a geodataframe\n",
    "states = gpd.read_file(\"data/pjm_system_map_export/states.geojson\")\n",
    "print(\"geojson file has been read as a geodataframe!\")\n",
    "\n",
    "# replace NULL and None values with NaN\n",
    "states = states.replace({\"Null\": np.nan, None: np.nan, \"\":np.nan})\n",
    "\n",
    "# select states that are in PJM states\n",
    "states = states[states.ABBREVIATION.isin([\"DE\", \"IL\", \"IN\", \"KY\", \"MD\", \"MI\", \"NJ\", \"NC\", \"OH\", \"PA\", \"TN\", \"VA\", \"WV\", \"DC\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get substation in lines data\n",
    "substations = substations[substations.SUBSTATION_GLOBALID.isin(lines.SUBSTATION_A_GLOBALID) | substations.SUBSTATION_GLOBALID.isin(lines.SUBSTATION_B_GLOBALID)]\n",
    "\n",
    "ax = states.plot(figsize=(30, 10))\n",
    "pjm_zone.geometry.plot(ax=ax, color=\"green\")\n",
    "lines.geometry.plot(ax=ax, color=\"yellow\")\n",
    "# substations.geometry.plot(ax=ax, color=\"red\")\n",
    "substations[~substations.PLANNING_ZONE_NAME.isnull()].geometry.plot(ax=ax, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "load = pd.read_csv(\"data/hrl_load_metered_2015.csv\")\n",
    "load.datetime_beginning_utc = pd.to_datetime(load.datetime_beginning_utc)\n",
    "load.datetime_beginning_ept = pd.to_datetime(load.datetime_beginning_ept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1, use tranmission zone\n",
    "# TODO: in the future, use load area, which requires further mapping based on EDC service areas\n",
    "# TODO: in the future, use voronoi cell for better approximation\n",
    "\n",
    "# get matching for load zone matching\n",
    "mapping = {\n",
    "    'Eastern Kentucky Power Cooperative': 'EKPC',\n",
    "    'Duke Energy Ohio Kentucky': 'DEOK',\n",
    "    'Commonwealth Edison Company': 'CE',\n",
    "    'American Electric Power Co., Inc.': 'AEP',\n",
    "    'American Transmission Systems, Inc.': 'ATSI',\n",
    "    'Allegheny Power': 'AP',\n",
    "    'Duquesne Light Company': 'DUQ',\n",
    "    'Public Service Electric and Gas Company': 'PS',\n",
    "    'Pennsylvania Electric Company': 'PN',\n",
    "    'PECO Energy Company': 'PE',\n",
    "    'Virginia Electric and Power Co.': 'DOM',\n",
    "    'PPL Electric Utilities Corporation': 'PL',\n",
    "    'Potomac Electric Power Company': 'PEP',\n",
    "    'The Dayton Power and Light Co.': 'DAY',\n",
    "    'Delmarva Power and Light Company': 'DPL',\n",
    "    'Jersey Central Power and Light Company': 'JC',\n",
    "    'Rockland Electric Company': 'RECO',\n",
    "    'Metropolitan Edison Company': 'ME',\n",
    "    'Baltimore Gas and Electric Company': 'BC',\n",
    "    'Atlantic City Electric Company': 'AE',\n",
    "    'Ohio Valley Electric Corporation': 'OVEC'\n",
    "}\n",
    "\n",
    "# check mapping\n",
    "# zones = ['PS', 'PE', 'PL', 'BC', 'JC', 'ME', 'PN', 'PEP', 'AE',\n",
    "#          'DPL', 'RECO', 'AP', 'CE', 'AEP', 'DAY', 'DUQ', 'DEOK',\n",
    "#          'ATSI', 'EKPC', 'DOM']\n",
    "\n",
    "for x in load.zone.unique():\n",
    "    if x not in mapping.values():\n",
    "        print(x)\n",
    "        \n",
    "print()\n",
    "\n",
    "for x in mapping.values():\n",
    "    if x not in load.zone.unique():\n",
    "        print(x)\n",
    "\n",
    "\n",
    "# replace commercial zones in substations\n",
    "substations.COMMERCIAL_ZONE = substations.COMMERCIAL_ZONE.replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct substation loads dataframe\n",
    "# substations = substations[~substations.COMMERCIAL_ZONE.isin([\"Null\", \"OVEC\"])]\n",
    "\n",
    "load_dict = {x:{} for x in substations.SUBSTATION_KEY}\n",
    "\n",
    "# figure out share of the load for each substation\n",
    "# TODO: currently equally divided between substations in a zone. Use Voronoi cells in the future.\n",
    "substations[\"loadAreaShare\"] = None\n",
    "\n",
    "for z in substations.COMMERCIAL_ZONE.unique():\n",
    "    tmp = substations[substations.COMMERCIAL_ZONE == z]\n",
    "    num_sub = tmp.shape[0]\n",
    "    substations.loc[tmp.index, \"loadAreaShare\"] = 1/num_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct substation loads dataframe\n",
    "for index, row in substations.iterrows():\n",
    "    subKey = row[\"SUBSTATION_KEY\"]\n",
    "    loadAreaShare = row[\"loadAreaShare\"]\n",
    "    zone = row[\"COMMERCIAL_ZONE\"]\n",
    "    \n",
    "    # if commercial zone is OVEC, skip\n",
    "    # TODO\n",
    "    if zone == \"OVEC\" or zone == \"Null\":\n",
    "        continue\n",
    "    \n",
    "    estimatedLoad = load[load.zone != \"RTO\"].groupby([\"zone\", \"datetime_beginning_utc\"]).sum().loc[zone] * loadAreaShare\n",
    "    load_dict[subKey] = estimatedLoad.to_dict()['mw']\n",
    "\n",
    "# construct DataFrame\n",
    "pypsa_load = pd.DataFrame(load_dict)\n",
    "\n",
    "# fillna\n",
    "pypsa_load = pypsa_load.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load attachment method 2\n",
    "# # TODO: voronoi cells\n",
    "\n",
    "\n",
    "# # get dict of zonal information in the form of {zone: geometry}\n",
    "# zonal_area = {row[\"PLANNING_ZONE_NAME\"]:row[\"geometry\"] for _, row in pjm_zone.iterrows()}\n",
    "\n",
    "# # reindex substations\n",
    "# substations.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # add a column to store within search info\n",
    "# substations[\"geometryZone\"] = None\n",
    "\n",
    "# # check zone\n",
    "# for index, row in substations.iterrows():\n",
    "#     point = row[\"geometry\"]\n",
    "#     for zone, shape in zonal_area.items():\n",
    "#         if point.within(shape):\n",
    "#             substations.loc[index, \"geometryZone\"] = zone\n",
    "#             break\n",
    "\n",
    "# # drop those without a geometryZone\n",
    "# substations = substations.dropna(subset=[\"geometryZone\"], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # intialize list to store voronoi area shape\n",
    "# voronoiGeometry = [None for _ in range(substations.shape[0])]\n",
    "# voronoiArea = [None for _ in range(substations.shape[0])]\n",
    "\n",
    "# for z in pjm_zone.PLANNING_ZONE_NAME.unique():\n",
    "#     # get list of substations\n",
    "#     tmp = substations[substations.geometryZone == z]\n",
    "    \n",
    "#     # if shape 0, skip (OVEC)\n",
    "#     # TODO: need to deal with this later\n",
    "#     if tmp.shape[0] == 0:\n",
    "#         continue\n",
    "    \n",
    "#     # get shape of the zone\n",
    "#     area_shape = zonal_area[z]\n",
    "    \n",
    "#     # get list of substation points\n",
    "#     pts = tmp.geometry.to_list()\n",
    "    \n",
    "#     # convert to coordinates\n",
    "#     coords = points_to_coords(pts)\n",
    "    \n",
    "#     # calculate Voronoi cells\n",
    "#     poly_shapes, pts, poly_to_pt_assignments = voronoi_regions_from_coords(coords, area_shape)\n",
    "#     poly_areas = calculate_polygon_areas(poly_shapes, m2_to_km2=True)\n",
    "    \n",
    "#     # store polyshape associated with a point in the dataframe\n",
    "#     for i, value in enumerate(poly_to_pt_assignments):\n",
    "#         index = tmp.index[value][0]\n",
    "#         voronoiGeometry[index] = poly_shapes[i]\n",
    "#         voronoiArea[index] = poly_areas[i]\n",
    "\n",
    "# # add Voronoi area to the dataframe\n",
    "# substations[\"voronoiGeometry\"] = gpd.GeoSeries(voronoiGeometry)\n",
    "# substations[\"voronoiArea\"] = pd.Series(voronoiArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA/QC\n",
    "# TODO: need to check voronoiArea and voronoiGeometry are correctly in place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat DataFrames for PyPSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define projection\n",
    "\n",
    "project = partial(\n",
    "    pyproj.transform,\n",
    "    pyproj.Proj('epsg:3857'), # source coordinate system\n",
    "    pyproj.Proj('epsg:4326')) # destination coordinate system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format substation data for PyPSA as buses\n",
    "\n",
    "# load dataframe, change and add some columns\n",
    "# TODO: currently simply dropping all duplicates\n",
    "pypsa_sub = substations.drop_duplicates().copy(deep=True)\n",
    "pypsa_sub[\"frequency\"] = 60\n",
    "pypsa_sub[\"type\"] = \"substation\"\n",
    "\n",
    "# convert spatial reference to coordinates\n",
    "pypsa_sub.geometry = pypsa_sub.geometry.apply(lambda x: transform(project, x))\n",
    "\n",
    "# format dataframe for outputing\n",
    "pypsa_sub[\"x\"] = pypsa_sub.geometry.y\n",
    "pypsa_sub[\"y\"] = pypsa_sub.geometry.x #pypsa_sub.y.apply(lambda x: '{:.4f}'.format(x))\n",
    "pypsa_sub[\"wkt_srid\"] = \"SRID=4326;POINT(\" + pypsa_sub.x.astype(str) + \" \" + pypsa_sub.y.astype(str) + \")\"\n",
    "pypsa_sub = pypsa_sub[[\"SUBSTATION_KEY\", \"frequency\", \"COMMERCIAL_ZONE\", \"NAME\", \"type\", \"VOLTAGE\", \"wkt_srid\", \"x\", \"y\"]]\n",
    "pypsa_sub.columns = [\"name\", \"frequency\", \"operator\", \"osm_name\", \"type\", \"voltage\", \"wkt_srid\", \"x\", \"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# format lines\n",
    "\n",
    "# get SUBSTATION_GLOBALID and SUBSTATION_KEY key_value pair\n",
    "sub_dict = substations[[\"SUBSTATION_GLOBALID\", \"SUBSTATION_KEY\", ]].drop_duplicates().set_index(\"SUBSTATION_GLOBALID\").to_dict(orient=\"index\")\n",
    "\n",
    "# initialize key\n",
    "pypsa_lines = {\"name\":[], \"bus0\":[], \"bus1\":[], \"frequency\":[], \"length\":[], \"operator\":[],\n",
    "               \"osm_name\":[], \"voltage\":[]\n",
    "              }\n",
    "\n",
    "# iterate over rows\n",
    "for index, row in lines.iterrows():\n",
    "    try:\n",
    "        name = row[\"TRANSMISSION_LINE_KEY\"]\n",
    "        bus0 = sub_dict[row[\"SUBSTATION_A_GLOBALID\"]][\"SUBSTATION_KEY\"]\n",
    "        bus1 = sub_dict[row[\"SUBSTATION_B_GLOBALID\"]][\"SUBSTATION_KEY\"]\n",
    "        frequency = 60\n",
    "        length = row[\"LENGTH_KM\"]\n",
    "        operator = row[\"NAME\"]\n",
    "        osm_name = row[\"LINE_ID\"]\n",
    "        voltage = row[\"VOLTAGE\"]\n",
    "\n",
    "        pypsa_lines[\"name\"].append(name)\n",
    "        pypsa_lines[\"bus0\"].append(bus0)\n",
    "        pypsa_lines[\"bus1\"].append(bus1)\n",
    "        pypsa_lines[\"frequency\"].append(frequency)\n",
    "        pypsa_lines[\"length\"].append(length)\n",
    "        pypsa_lines[\"operator\"].append(operator)\n",
    "        pypsa_lines[\"osm_name\"].append(osm_name)\n",
    "        pypsa_lines[\"voltage\"].append(voltage)\n",
    "    except:\n",
    "        print(\"can't append line: TRANSMISSION_LINE_KEY\", row[\"TRANSMISSION_LINE_KEY\"])\n",
    "\n",
    "# initialize dataframe from dict\n",
    "pypsa_lines = pd.DataFrame(pypsa_lines)\n",
    "\n",
    "# drop lines where length is null\n",
    "# TODO: find some other way in the future\n",
    "# TODO: perhaps apporximate using path geometry or euclidean distance between substations\n",
    "pypsa_lines = pypsa_lines[pypsa_lines.length != \"Null\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format generators for PyPSA\n",
    "\n",
    "# copy queue_merged into a geneartor dataframe\n",
    "pypsa_gens = queue_merged[[\"Name\", \"MW In Service\", \"Fuel\", \"geometry\"]].copy(deep=True)\n",
    "pypsa_gens[\"substation\"] = \"\"\n",
    "\n",
    "# get subs that show up in lines\n",
    "lst_of_subs = substations[substations.SUBSTATION_KEY.isin(pypsa_lines.bus0) | substations.SUBSTATION_KEY.isin(pypsa_lines.bus1)].copy(deep=True)\n",
    "lst_of_subs[\"tmp_dist\"] = None\n",
    "\n",
    "# associate generators to closet subs\n",
    "for index, row in pypsa_gens.iterrows():\n",
    "    pt = row[\"geometry\"]\n",
    "    lst_of_subs[\"tmp_dist\"] = lst_of_subs.geometry.apply(lambda x: pt.distance(x))\n",
    "    \n",
    "    min_sub = lst_of_subs[lst_of_subs.tmp_dist == min(lst_of_subs.tmp_dist)]\n",
    "    min_sub = min_sub.SUBSTATION_KEY.to_list()[0]\n",
    "    \n",
    "    pypsa_gens.loc[index, \"substation\"] = min_sub\n",
    "\n",
    "# reformat columns for pypsa\n",
    "pypsa_gens = pypsa_gens[[\"Name\", \"substation\", \"MW In Service\", \"Fuel\"]]\n",
    "pypsa_gens.columns = [\"name\", \"bus\", \"p_nom\", \"carrier\"]\n",
    "\n",
    "# get marginal cost\n",
    "pypsa_gens[\"marginal_cost\"] = 30\n",
    "\n",
    "# drop null rows\n",
    "# TODO: better handling in the future\n",
    "pypsa_gens.dropna(inplace=True)\n",
    "\n",
    "# set p_max_pu\n",
    "# TODO: set the actual p_max_pu for renewable\n",
    "# pypsa_gens[\"p_max_pu\"] = 1\n",
    "\n",
    "# deal with duplicates\n",
    "# TODO: figure out why they are duplicates in the future\n",
    "pypsa_gens.drop_duplicates(subset=\"name\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_merged[queue_merged.Name == \"Arnold 115kV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substations[substations.SUBSTATION_KEY == \"79429\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# QA/QC on gen-substation approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_gens.groupby(\"carrier\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(\"data/gen_by_fuel_2019.csv\")\n",
    "tmp.datetime_beginning_utc = pd.to_datetime(tmp.datetime_beginning_utc)\n",
    "tmp = tmp[[\"datetime_beginning_utc\", \"fuel_type\", \"mw\"]]\n",
    "\n",
    "tmp.pivot(index=\"datetime_beginning_utc\", columns=\"fuel_type\", values=\"mw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export csv\n",
    "\n",
    "pypsa_dir = \"data/pjm_system_map_pypsa\"\n",
    "\n",
    "# export dataframes\n",
    "pypsa_sub.to_csv(os.path.join(pypsa_dir, \"buses.csv\"), index=False)\n",
    "pypsa_lines.to_csv(os.path.join(pypsa_dir, \"lines.csv\"), index=False)\n",
    "pypsa_gens.to_csv(os.path.join(pypsa_dir, \"generators.csv\"), index=False)\n",
    "pypsa_load.to_csv(os.path.join(pypsa_dir, \"loads-p_set.csv\"))\n",
    "\n",
    "# generate load name matching\n",
    "tmp = pd.DataFrame(columns=[\"name\", \"bus\"])\n",
    "tmp[\"name\"] = pypsa_sub[\"name\"]\n",
    "tmp[\"bus\"] = pypsa_sub[\"name\"]\n",
    "tmp.to_csv(os.path.join(pypsa_dir, \"loads.csv\"), index=False)\n",
    "\n",
    "\n",
    "# generate snapshots\n",
    "tmp = pd.DataFrame(columns=[\"name\", \"weights\"])\n",
    "tmp[\"name\"] = pypsa_load.index\n",
    "tmp[\"weights\"] = 1\n",
    "tmp.to_csv(os.path.join(pypsa_dir, \"snapshots.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start PyPSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get PyPSA Network\n",
    "\n",
    "csv_folder_name = \"data/pjm_system_map_pypsa\"\n",
    "\n",
    "network = pypsa.Network(csv_folder_name=csv_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
    "\n",
    "fig.set_size_inches(10,10)\n",
    "\n",
    "load_distribution = network.loads_t.p_set.loc[network.snapshots[0]].groupby(network.loads.bus).sum()\n",
    "\n",
    "network.plot(bus_sizes=0.00005*load_distribution,ax=ax,title=\"Load distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_gens.carrier.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
    "fig.set_size_inches(10,10)\n",
    "\n",
    "gens = network.generators[network.generators.carrier == \"Natural Gas\"]\n",
    "gen_distribution = gens.groupby(\"bus\").sum()[\"p_nom\"].reindex(network.buses.index,fill_value=0.)\n",
    "\n",
    "network.plot(bus_sizes=0.0001*gen_distribution,ax=ax,title=\"Generation distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.lines.s_nom = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import export\n",
    "# generator capacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.generators.p_nom.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.loads_t[\"p_set\"].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.lines.s_nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 4\n",
    "\n",
    "solver_name = \"glpk\"\n",
    "\n",
    "print(\"Performing linear OPF for one day, {} snapshots at a time:\".format(group_size))\n",
    "\n",
    "network.storage_units.state_of_charge_initial = 0.\n",
    "\n",
    "for i in range(int(24/group_size)):\n",
    "    #set the initial state of charge based on previous round\n",
    "    network.lopf(network.snapshots[group_size*i:group_size*i+group_size], solver_name=solver_name)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power_simulation",
   "language": "python",
   "name": "power_simulation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
